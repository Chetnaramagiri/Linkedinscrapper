{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "6d8cf0e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (4.18.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.1.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from selenium) (0.24.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from trio~=0.17->selenium) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "4cec0538-342d-46ba-a310-7736b016520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Section imports aree the libraries that will be used in the code\n",
    "\n",
    "# selenium==4.16.0\n",
    "\n",
    "from selenium import webdriver # to open a web browser\n",
    "from selenium.webdriver.common.keys import Keys # to be able to type in the browser\n",
    "from selenium.webdriver.common.by import By # to be able to search things in the browser\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup # to be able to search the HTML in browser \n",
    "import time # to be able to wait for x amount of time seconds. \n",
    "\n",
    "\n",
    "linkedin_email =  \"chetnagirish2@gmail.com\"\n",
    "linkedin_password = \"Boss1234$\"\n",
    "\n",
    "url = 'https://www.linkedin.com/jobs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d9557-7c41-4d60-b55e-69dfb93a195e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e30ba-b53c-4f21-ba09-0f97adb4b28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "acc538ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This section contains the code to scrape the data. specifically the salary.\n",
    "\n",
    "import re\n",
    "\n",
    "def check_element (elem):\n",
    "    if elem is None: \n",
    "        return ''\n",
    "    if len (elem) > 0:\n",
    "        return elem.text.strip()\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "\n",
    "def extract_hourly_or_salary_range(salary_string):\n",
    "    hourly_match = re.search(r'\\$([\\d,]+(?:\\.\\d+)?)\\s?(?:-|to)?\\s?\\$?([\\d,]+(?:\\.\\d+)?)?\\s?\\/\\s?hr', salary_string)\n",
    "    salary_match = re.search(r'\\$([\\d,]+(?:\\.\\d+)?)\\s?K?\\s?\\/\\s?yr\\s?(?:-|to)?\\s?\\$?([\\d,]+(?:\\.\\d+)?)?\\s?K?\\s?\\/\\s?yr', salary_string)\n",
    "\n",
    "    if hourly_match:\n",
    "        hourly_rate_lower = float(hourly_match.group(1).replace(',', ''))\n",
    "        hourly_rate_upper = float(hourly_match.group(2).replace(',', '')) if hourly_match.group(2) else hourly_rate_lower\n",
    "\n",
    "        # Assuming 40 hours per week and 52 weeks per year\n",
    "        annual_lower = hourly_rate_lower * 40 * 52\n",
    "        annual_upper = hourly_rate_upper * 40 * 52\n",
    "\n",
    "        average = (annual_lower + annual_upper) / 2\n",
    "\n",
    "        return [annual_lower, annual_upper, average]\n",
    "    elif salary_match:\n",
    "        salary_lower = float(salary_match.group(1).replace(',', '')) * 1000\n",
    "        salary_upper = float(salary_match.group(2).replace(',', '')) * 1000 if salary_match.group(2) else salary_lower\n",
    "\n",
    "        average = (salary_lower + salary_upper) / 2\n",
    "\n",
    "        return [salary_lower, salary_upper, average]\n",
    "    else:\n",
    "        return [0,0,0]\n",
    "\n",
    "# # Example usage\n",
    "# strings = [\n",
    "#     '$57/hr - $78/hr · Vision, +1 benefit',\n",
    "#     '401(k), +6 benefits',\n",
    "#     '$117K/yr - $134.2K/yr · Medical benefit',\n",
    "#     '$55.1K/yr - $68.9K/yr',\n",
    "#     '$70/hr - $75/hr · 401(k), +1 benefit',\n",
    "#     'Starting at $40/hr',\n",
    "#     '$57/hr - $78/hr · Vision, +1 benefit'\n",
    "# ]\n",
    "\n",
    "# for salary_string in strings:\n",
    "#     result = extract_hourly_or_salary_range(salary_string)\n",
    "#     if result:\n",
    "#         print(f\"Input: {salary_string}\\nConverted Range: {result}\\n\")\n",
    "#     else:\n",
    "#         print(f\"Input: {salary_string}\\nUnable to extract hourly rate or salary range.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "991d43de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  THis section of the code is to start the browser and go the the respective URL. and located the box for user id and password\n",
    "browser = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install()))# start the browser \n",
    "browser.get(url)  # in browser search the URL \n",
    "\n",
    "email_input = browser.find_element(\"name\", \"session_key\")\n",
    "password_input = browser.find_element(\"name\", \"session_password\")\n",
    "login_button = browser.find_element(\"xpath\", \"//button[@type='submit']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "5688cbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This section of the code is to enter the user id and password\n",
    "email_input.send_keys(linkedin_email)\n",
    "password_input.send_keys(linkedin_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "2a4690a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Click the login button\n",
    "login_button.click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "cff289a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plug in Job title in the search box\n",
    "search_box = browser.find_element(By.CLASS_NAME, 'jobs-search-box__text-input')\n",
    "search_box.send_keys('Data Analyst')\n",
    "search_box.send_keys(Keys.RETURN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "f86fc4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# locate and clik the date filter\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Wait for the element to be visible\n",
    "WebDriverWait(browser, 10).until(EC.visibility_of_element_located((By.ID, \"searchFilter_timePostedRange\")))\n",
    "\n",
    "date_filter = browser.find_element(By.ID, \"searchFilter_timePostedRange\")\n",
    "date_filter.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "18f37914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# locate week filter on the date filter\n",
    "\n",
    "class_name = \"t-14 t-black--light t-normal\"\n",
    "span_text = \"week\"\n",
    "\n",
    "past_week_x_path= f\"//span[@class='{class_name}' and contains(.,'{span_text}')]\"\n",
    "past_week = browser.find_element(\"xpath\", past_week_x_path)\n",
    "past_week.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "3af0bf66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# click the show results button the the date filter\n",
    "class_name = \"t-14 t-black--light t-normal\"\n",
    "span_text = \"week\"\n",
    "\n",
    "past_week_x_path= f\"//span[@class='{class_name}' and contains(.,'{span_text}')]\"\n",
    "past_week = browser.find_element(\"xpath\", past_week_x_path)\n",
    "past_week.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "2c26413c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find all the job postings on the wbsite \n",
    "jobs_block = browser.find_element(By.CSS_SELECTOR, '.scaffold-layout__list-container')\n",
    "jobs_list= jobs_block.find_elements(By.CSS_SELECTOR, '.jobs-search-results__list-item')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "45e4174d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First job details: Data Analyst \n",
      "Zolon Tech Inc.\n",
      "Cupertino, CA (Hybrid)\n",
      "Viewed\n",
      "Promoted\n",
      "Easy Apply\n"
     ]
    }
   ],
   "source": [
    "# scroll down the page\n",
    "# NEXT STEP : Find the most efficient way to scroll down the list so all the job profiles are loaded. \n",
    "x = 0\n",
    "while (x<=len(jobs_list)-1):\n",
    "        # Print job element\n",
    "\n",
    "\n",
    "    browser.execute_script(\"arguments[0].scrollIntoView();\", jobs_list[x])\n",
    "    x= x+1\n",
    "    time.sleep(1)\n",
    "first_job_element = jobs_list[0]\n",
    "\n",
    "# Print job details\n",
    "print(\"First job details:\", first_job_element.text)\n",
    "\n",
    "    \n",
    "\n",
    "# Ignore the bottom of this section \n",
    "# browser.execute_script(\"arguments[0].scrollIntoView();\", jobs_list[len(jobs_list)-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "cb0f8d95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse the HTML in \n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(browser.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "6f012e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# find all the LI elements in the HTML\n",
    "target_class = 'jobs-search-results'\n",
    "target_li_elements = soup.find_all('li', class_=lambda x: x and 'jobs-search-results__list-item' in x)\n",
    "# target_li_elements = soup.find_all('li', class_=lambda x: x and 'jobs-search-results__list-item' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "5e4ba58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_title': 'Business Data Analyst', 'company': 'Indotronix Avani Group', 'location': 'St Petersburg, FL (On-site)', 'salary_string': '', 'lower_bound_extracted_salary': 0, 'upper_extracted_salary': 0, 'average_extracted_salary': 0, 'link': 'www.linkedin.com/jobs/view/3834873605/?eBP=CwEAAAGOZHBY-Ads0IBdDw-Eh7frY94G1_WuqglxIG-EJV2SjZ5fR13rMA2mbk9DPaf0f4VgQ0NUk5iOYf8i847svNAWyzixvuoe03t42q_zu-Gd6oBC6av9RR6GaYge2me2MUVqc_uYyFvtANnJ2EXtYdq73ObAM4AkL4RphHNN_0IKZesOnFhWKRp9JugyehcfpNc4mpvcnZZA1UlMNSTrLrAC-g441Uix_rRfx7ZQlIT4I2tEJHa_FeXEt5NAtczg7Il9p9-jgEMGtiPBTcm16OD_k6-dbLMbgQC6dok3c1fp-asF2YbVKY5SPTsX8ISBQcbu9vzhIVb6vYbQzLDSmsQ8RG7SWQ-1oAJurjVkGNjh6-Qkcu4SRMVein3PxSiHlWbEkfvcBIiQQv4tvEh7TIOaKcbMRZ0&refId=KB3iwr%2FES9gTgxjmGZbXkQ%3D%3D&trackingId=UD5OLV%2BsP7yFNnJW%2BOrVpw%3D%3D&trk=flagship3_search_srp_jobs'}\n"
     ]
    }
   ],
   "source": [
    "# Testing the code to see if LI element truely has the data we need to extract \n",
    "#  NEXT STEP: Write a loop the iterate through all the LI elements and check if the data we need is in the LI element. Append the date in to an array\n",
    "#  Import the data in to dataframe. \n",
    "\n",
    "num=15\n",
    "\n",
    "\n",
    "job_title = check_element (target_li_elements[num].find('a', class_=lambda x: x and 'title' in x))\n",
    "company = check_element ( target_li_elements[num].find('span', class_= 'job-card-container__primary-description') )\n",
    "location = check_element (target_li_elements[num].find('div', class_= 'artdeco-entity-lockup__caption ember-view') )\n",
    "salary = check_element (target_li_elements[num].find('div', class_= lambda x: x and 'mt1 t-sans t-12 t-black--light' in x) )\n",
    "link = target_li_elements[num].find('a').get('href')\n",
    "\n",
    "dic = {     'job_title': job_title ,\n",
    "            'company': company,\n",
    "            'location': location,\n",
    "            'salary_string': salary,\n",
    "            'lower_bound_extracted_salary':  (extract_hourly_or_salary_range(salary)[0]),\n",
    "            'upper_extracted_salary':  (extract_hourly_or_salary_range(salary)[1]),\n",
    "            'average_extracted_salary':  (extract_hourly_or_salary_range(salary)[2]),\n",
    "            'link': 'www.linkedin.com'+link\n",
    "       \n",
    "          }\n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "45ca3f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: find the page 2 and 3 and 4 and iterate iteratively go through untill the very last apge. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "74fcf6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading page - 2\n",
      "loading page - 3\n",
      "loading page - 4\n",
      "loading page - 5\n",
      "loading page - 6\n",
      "loading page - 7\n",
      "loading page - 8\n",
      "loading page - …\n",
      "loading page - 10\n",
      "loading page - 11\n",
      "loading page - 12\n",
      "loading page - 13\n",
      "loading page - 14\n",
      "loading page - 15\n",
      "loading page - 16\n",
      "loading page - 17\n",
      "loading page - 18\n",
      "loading page - 19\n",
      "loading page - 20\n",
      "loading page - 21\n",
      "loading page - 22\n",
      "loading page - 23\n",
      "loading page - 24\n",
      "loading page - 25\n",
      "loading page - 26\n",
      "loading page - 27\n",
      "loading page - 28\n",
      "loading page - 29\n",
      "loading page - 30\n",
      "loading page - 31\n",
      "loading page - 32\n",
      "loading page - 33\n",
      "loading page - 34\n",
      "loading page - 35\n",
      "loading page - 36\n",
      "loading page - 37\n",
      "loading page - 38\n",
      "loading page - 39\n",
      "loading page - 40\n",
      "scanned all the pages\n"
     ]
    }
   ],
   "source": [
    "# Step 3: find the page 2 and 3 and 4 and iterate iteratively go through untill the very last apge. \n",
    "next_page_num = 1\n",
    "while  True:\n",
    "    # find the current page\n",
    "    current_page_xpath = \"//button[@aria-current='true']\"\n",
    "    current_page = int(browser.find_element(\"xpath\", current_page_xpath).text)\n",
    "    # find the next page number\n",
    "    next_page_num = current_page+1\n",
    "    # find it on the webpage\n",
    "    next_page_xpath = \"//button[@aria-label='Page \"+str(next_page_num)+\"']\"\n",
    "    try:\n",
    "        next_page = browser.find_element(\"xpath\", next_page_xpath)\n",
    "        # next_page = WebDriverWait(browser, 10).until(EC.element_to_be_clickable((By.XPATH, next_page_xpath)))\n",
    "        print (\"loading page - \" + str(next_page.text))\n",
    "        # go to the next page\n",
    "        next_page.click()\n",
    "        # wait for the page to load\n",
    "        time.sleep(3)\n",
    "    except :\n",
    "        print ('scanned all the pages')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "c482fee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chetn\\anaconda_new\\envs\\notebook-7.0.8\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "feabd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import pandas as pd\n",
    "\n",
    "#df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "a278dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job_title': 'Data Analyst', 'company': 'Insight Global', 'location': 'Charlotte, NC (Hybrid)', 'salary_string': '$60.56/hr', 'lower_bound_extracted_salary': 125964.8, 'upper_extracted_salary': 125964.8, 'average_extracted_salary': 125964.8, 'link': 'www.linkedin.com/jobs/view/3852177580/?eBP=CwEAAAGOZHBY-Zj1rel8Hjy_3Kl1K9Dn_IdcwkMEoFvSUUemAnEuWQzy2x1HG1ju-qHgxsKfXvIrvKLecrvfVVdxZFNJZqH13mZ-7NjCSLMaHgTb8utYtzpYr9kDx_eq5ZzK7sdfZ1pj7CVTbJZn-KGVzxh0B8I2blMBhtmzXmQ_5VMa8Q1BF6IGbN6EXnVs9y39yHvNPN6vQBVxLUwulneq82QpNJktz9onPICLo0NvaG4-_KWlHKWNyT58FReAa1_SfQaHwtq63DgD7aD2LnPeTdbIul9JputuOmaWGp6HqOGAQcZd6mBCregVGXzuTvxlVBxzsRHjNdkgYHWi1R8GELAW47wqZrtl4DE757BXAUa6RttcMLRfhf-D8vFNAg9s1R1h3SRaTADYzfeU-kFX7GvoU8gqIgI&refId=KB3iwr%2FES9gTgxjmGZbXkQ%3D%3D&trackingId=kLD4e9c7ADAkW9jozFmE%2Fg%3D%3D&trk=flagship3_search_srp_jobs'}\n"
     ]
    }
   ],
   "source": [
    "num=24\n",
    "\n",
    "\n",
    "job_title = check_element (target_li_elements[num].find('a', class_=lambda x: x and 'title' in x))\n",
    "company = check_element ( target_li_elements[num].find('span', class_= 'job-card-container__primary-description') )\n",
    "location = check_element (target_li_elements[num].find('div', class_= 'artdeco-entity-lockup__caption ember-view') )\n",
    "salary = check_element (target_li_elements[num].find('div', class_= lambda x: x and 'mt1 t-sans t-12 t-black--light' in x) )\n",
    "link = target_li_elements[num].find('a').get('href')\n",
    "\n",
    "dic = {     'job_title': job_title ,\n",
    "            'company': company,\n",
    "            'location': location,\n",
    "            'salary_string': salary,\n",
    "            'lower_bound_extracted_salary':  (extract_hourly_or_salary_range(salary)[0]),\n",
    "            'upper_extracted_salary':  (extract_hourly_or_salary_range(salary)[1]),\n",
    "            'average_extracted_salary':  (extract_hourly_or_salary_range(salary)[2]),\n",
    "            'link': 'www.linkedin.com'+link\n",
    "       \n",
    "          }\n",
    "\n",
    "print(dic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "194ab0a0-2748-4c06-91db-e3849f83976e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (target_li_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "33074dfc-141b-4cee-8a51-722263a26f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#def scrape_job_data_for_page(target_li_elements):\\n   # jobs_data = []\\n\\n     #for num in range(len(target_li_elements)):\\n       #  job_title = check_element(target_li_elements[num].find(\\'span\\', class_=\\'job-details-jobs-unified-top-card_job-title-link\\'))         \\n         #location = check_element(target_li_elements[num].find(\\'div\\', class_=\\'jobs-details-jobs-unified-top-card_primary-description-without-tagline mbs\\')) \\n  #       salary = check_element(target_li_elements[num].find(\\'li\\', class_=\\'jobs-details-jobs-unified-top-card_job-details-jobs-unified-top-card_job-insight-highlight\\'))\\n   \\n    #     link = target_li_elements[num].find(\\'a\\').get(\\'href\\')\\n\\n      #   try:\\n            lower_bound, upper_bound, average = extract_hourly_or_salary_range(salary)\\n        # except Exception as e:\\n       #      print(f\"Error extracting salary from \\'{salary}\\': {e}\")\\n         #    lower_bound, upper_bound, average = None, None, None\\n\\n   #      dic = {\\n         \\'job_title\\': job_title,\\n         \\'company\\': company,\\n            \\'location\\': location,\\n            \\'salary_string\\': salary,\\n            \\'lower_bound_extracted_salary\\': lower_bound,\\n            \\'upper_extracted_salary\\': upper_bound,\\n            \\'average_extracted_salary\\': average,\\n            \\'link\\': \\'www.linkedin.com\\' + link\\n        }\\n\\n        jobs_data.append(dic)\\n\\n return jobs_data\\n\\n# Example function to fetch target_li_elements for a given page number\\ndef get_target_li_elements_for_page(page_number):\\n    # This function should fetch and return the target li elements for a given page number\\n    # Implement this function according to your web scraping setup\\n    pass\\n\\n# Example code to scrape job data from multiple pages\\ntotal_pages = 40  # Example total number of pages to scrape\\n\\nfor page_number in range(1, total_pages + 1):\\n    target_li_elements = get_target_li_elements_for_page(page_number)\\n    if target_li_elements:\\n        page_jobs_data = scrape_job_data_for_page(target_li_elements)\\n        jobs_data.append(page_jobs_data)\\n\\nprint(\"Total jobs data collected:\", len(jobs_data))'"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#def scrape_job_data_for_page(target_li_elements):\n",
    "   # jobs_data = []\n",
    "\n",
    "     #for num in range(len(target_li_elements)):\n",
    "       #  job_title = check_element(target_li_elements[num].find('span', class_='job-details-jobs-unified-top-card_job-title-link'))         \n",
    "         #location = check_element(target_li_elements[num].find('div', class_='jobs-details-jobs-unified-top-card_primary-description-without-tagline mbs')) \n",
    "  #       salary = check_element(target_li_elements[num].find('li', class_='jobs-details-jobs-unified-top-card_job-details-jobs-unified-top-card_job-insight-highlight'))\n",
    "   \n",
    "    #     link = target_li_elements[num].find('a').get('href')\n",
    "\n",
    "      #   try:\n",
    "            lower_bound, upper_bound, average = extract_hourly_or_salary_range(salary)\n",
    "        # except Exception as e:\n",
    "       #      print(f\"Error extracting salary from '{salary}': {e}\")\n",
    "         #    lower_bound, upper_bound, average = None, None, None\n",
    "\n",
    "   #      dic = {\n",
    "         'job_title': job_title,\n",
    "         'company': company,\n",
    "            'location': location,\n",
    "            'salary_string': salary,\n",
    "            'lower_bound_extracted_salary': lower_bound,\n",
    "            'upper_extracted_salary': upper_bound,\n",
    "            'average_extracted_salary': average,\n",
    "            'link': 'www.linkedin.com' + link\n",
    "        }\n",
    "\n",
    "        jobs_data.append(dic)\n",
    "\n",
    " return jobs_data\n",
    "\n",
    "# Example function to fetch target_li_elements for a given page number\n",
    "def get_target_li_elements_for_page(page_number):\n",
    "    # This function should fetch and return the target li elements for a given page number\n",
    "    # Implement this function according to your web scraping setup\n",
    "    pass\n",
    "\n",
    "# Example code to scrape job data from multiple pages\n",
    "total_pages = 40  # Example total number of pages to scrape\n",
    "\n",
    "for page_number in range(1, total_pages + 1):\n",
    "    target_li_elements = get_target_li_elements_for_page(page_number)\n",
    "    if target_li_elements:\n",
    "        page_jobs_data = scrape_job_data_for_page(target_li_elements)\n",
    "        jobs_data.append(page_jobs_data)\n",
    "\n",
    "print(\"Total jobs data collected:\", len(jobs_data))\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "e955bf1a-f343-4eb9-b6b6-e66c69553ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "jobs_data = []\n",
    "for num in range (0,len (target_li_elements)-1):\n",
    "    job_title = check_element (target_li_elements[num].find('a', class_=lambda x: x and 'title' in x))\n",
    "    company = check_element ( target_li_elements[num].find('span', class_= 'job-card-container__primary-description') )\n",
    "    location = check_element (target_li_elements[num].find('div', class_= 'artdeco-entity-lockup__caption ember-view') )\n",
    "    salary = check_element (target_li_elements[num].find('div', class_= lambda x: x and 'mt1 t-sans t-12 t-black--light' in x) )\n",
    "    link = target_li_elements[num].find('a').get('href')\n",
    "\n",
    "    dic = {     'job_title': job_title ,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'salary_string': salary,\n",
    "                'lower_bound_extracted_salary':  (extract_hourly_or_salary_range(salary)[0]),\n",
    "                'upper_extracted_salary':  (extract_hourly_or_salary_range(salary)[1]),\n",
    "                'average_extracted_salary':  (extract_hourly_or_salary_range(salary)[2]),\n",
    "                'link': 'www.linkedin.com'+link\n",
    "        \n",
    "            }\n",
    "    jobs_data.append(dic)\n",
    "\n",
    "print(len(jobs_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766df203-57dd-4308-9913-ccbb70b33f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
